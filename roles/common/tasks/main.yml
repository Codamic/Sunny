- name: Installing Emacs25 NOX
  apt:
    name: emacs24-nox
    state: present
  tags: [common, emacs]

- name: Installing htop
  apt:
    name: htop
    state: present
  tags: [common, htop]

- name: Installing ntp
  apt:
    name: ntp
    state: present
  tags: [common, ntp]

- name: Copy the ntp.conf template file
  template:
    src: ntp.conf.j2
    dest: /etc/ntp.conf
  notify:
  - restart ntp
  tags: [ common, ntp ]

- name: Setting sysctl->fs.file-max
  sysctl:
    name: fs.file-max
    value: 150000
    state: present
    reload: yes

- name: Setting sysctl->kernel.pid_max
  sysctl:
    name: kernel.pid_max
    value:  4194303
    state: present
    reload: yes

- name: Increase range of ports that can be used
  sysctl:
    name: net.ipv4.ip_local_port_range
    value:  1024 65535
    state: present
    reload: yes



# https://tweaked.io/guide/kernel/
# Forking servers, like PostgreSQL or Apache, scale to much higher levels of concurrent connections if this is made larger
# kernel.sched_migration_cost_ns=5000000

# https://tweaked.io/guide/kernel/
# Various PostgreSQL users have reported (on the postgresql performance mailing list) gains up to 30% on highly concurrent workloads on multi-core systems
# kernel.sched_autogroup_enabled = 0

# https://github.com/ton31337/tools/wiki/tcp_slow_start_after_idle---tcp_no_metrics_save-performance
- name: Avoid falling back to slow start after a connection goes idle(1)
  sysctl:
    name:  net.ipv4.tcp_slow_start_after_idle
    value:  0
    state: present
    reload: yes

- name: Avoid falling back to slow start after a connection goes idle(2)
  sysctl:
    name:  net.ipv4.tcp_no_metrics_save
    value:  0
    state: present
    reload: yes

- name: Abort TCP on overflow
  sysctl:
    name:  net.ipv4.tcp_abort_on_overflow
    value:  0
    state: present
    reload: yes


# # (Use with caution according to the kernel documentation!)
# - name: Enables fast recycling of TIME_WAIT sockets.
#   sysctl:
#     name:  net.ipv4.tcp_tw_recycle
#     value:  1
#     state: present
#     reload: yes


# Allow reuse of sockets in TIME_WAIT state for new connections
# only when it is safe from the network stackâ€™s perspective.
# net.ipv4.tcp_tw_reuse = 1

- name: Turn on SYN-flood protections
  sysctl:
    name:  net.ipv4.tcp_syncookies
    value:  1
    state: present
    reload: yes


# Only retry creating TCP connections twice
# Minimize the time it takes for a connection attempt to fail
#net.ipv4.tcp_syn_retries=2
#net.ipv4.tcp_synack_retries=2
#net.ipv4.tcp_orphan_retries=2

# How many retries TCP makes on data segments (default 15)
# Some guides suggest to reduce this value
#net.ipv4.tcp_retries2=8

# Optimize connection queues
# https://www.linode.com/docs/web-servers/nginx/configure-nginx-for-optimized-performance
- name: Increase the number of packets that can be queued
  sysctl:
    name: net.core.netdev_max_backlog
    value:  3240000
    state: present
    reload: yes


- name: Max number of "backlogged sockets"
  sysctl:
    name: net.core.somaxconn
    value: 50000
    state: present
    reload: yes

- name: Increase max number of sockets allowed in TIME_WAIT
  sysctl:
    name: net.ipv4.tcp_max_tw_buckets
    value: 1440000
    state: present
    reload: yes

- name: Number of packets to keep in the backlog
  sysctl:
    name: net.ipv4.tcp_max_syn_backlog
    value: 3240000
    state: present
    reload: yes

# TCP memory tuning
# View memory TCP actually uses with: cat /proc/net/sockstat
# *** These values are auto-created based on your server specs ***
# *** Edit these parameters with caution because they will use more RAM ***
# Changes suggested by IBM on https://www.ibm.com/developerworks/community/wikis/home?lang=en#!/wiki/Welcome%20to%20High%20Performance%20Computing%20%28HPC%29%20Central/page/Linux%20System%20Tuning%20Recommendations
# Increase the default socket buffer read size (rmem_default) and write size (wmem_default)
# *** Maybe recommended only for high-RAM servers? ***
#net.core.rmem_default=16777216
#net.core.wmem_default=16777216
# Increase the max socket buffer size (optmem_max), max socket buffer read size (rmem_max), max socket buffer write size (wmem_max)
# 16MB per socket - which sounds like a lot, but will virtually never consume that much
# rmem_max over-rides tcp_rmem param, wmem_max over-rides tcp_wmem param and optmem_max over-rides tcp_mem param
#net.core.optmem_max=16777216
#net.core.rmem_max=16777216
#net.core.wmem_max=16777216
# Configure the Min, Pressure, Max values (units are in page size)
# Useful mostly for very high-traffic websites that have a lot of RAM
# Consider that we already set the *_max values to 16777216
# So you may eventually comment these three lines
#net.ipv4.tcp_mem=16777216 16777216 16777216
#net.ipv4.tcp_wmem=4096 87380 16777216
#net.ipv4.tcp_rmem=4096 87380 16777216

# Keepalive optimizations
# By default, the keepalive routines wait for two hours (7200 secs) before sending the first keepalive probe,
# and then resend it every 75 seconds. If no ACK response is received for 9 consecutive times, the connection is marked as broken.
# The default values are: tcp_keepalive_time = 7200, tcp_keepalive_intvl = 75, tcp_keepalive_probes = 9
# We would decrease the default values for tcp_keepalive_* params as follow:
#net.ipv4.tcp_keepalive_time = 600
#net.ipv4.tcp_keepalive_intvl = 10
#net.ipv4.tcp_keepalive_probes = 9

# The TCP FIN timeout belays the amount of time a port must be inactive before it can reused for another connection.
# The default is often 60 seconds, but can normally be safely reduced to 30 or even 15 seconds
# https://www.linode.com/docs/web-servers/nginx/configure-nginx-for-optimized-performance
#net.ipv4.tcp_fin_timeout = 7
